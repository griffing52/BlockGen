{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8cc1ae",
   "metadata": {},
   "source": [
    "# Large PyG Graph Generator (Scalable)\n",
    "\n",
    "This notebook trains a larger torch_geometric model that:\n",
    "- encodes full structure graphs with attention (`TransformerConv`)\n",
    "- learns a latent distribution (VAE-style)\n",
    "- autoregressively decodes block-token sequences\n",
    "- samples **new** graphs not memorized from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441dd34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "torch: 2.10.0+cu128\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from blockgen.utils.graph_data import SchematicGraphDataset, NODE_BLOCK, structure_to_pyg_data\n",
    "from blockgen.utils.data import Structure\n",
    "from blockgen.models import LargePyGGraphGenerator, LargePyGGraphGeneratorConfig\n",
    "from blockgen.renderer.render import render_schem\n",
    "\n",
    "SEED = 2026\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n",
    "print('torch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932cea4",
   "metadata": {},
   "source": [
    "## 1) Load full dataset as graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d642704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total schematics found: 10963\n",
      "graph dataset size: 10963\n"
     ]
    }
   ],
   "source": [
    "repo_root = Path.cwd()\n",
    "if not (repo_root / 'data' / 'raw').exists():\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "raw_dir = repo_root / 'data' / 'raw'\n",
    "all_paths = sorted(raw_dir.glob('*.schematic'))\n",
    "print('total schematics found:', len(all_paths))\n",
    "\n",
    "# Use the entire dataset by default.\n",
    "paths = [str(p) for p in all_paths]\n",
    "\n",
    "dataset = SchematicGraphDataset(\n",
    "    paths,\n",
    "    include_air=False,\n",
    "    crop_non_air=True,\n",
    "    max_dim=20,\n",
    ")\n",
    "\n",
    "print('graph dataset size:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a2059",
   "metadata": {},
   "source": [
    "## 2) Build block-token vocabulary from all graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc566413",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadGzipFile",
     "evalue": "Not a gzipped file (b'Ra')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadGzipFile\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m num_blocks_list = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     g = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m     block_ids = g.block_id[g.node_type == NODE_BLOCK].tolist()\n\u001b[32m      8\u001b[39m     block_counter.update(\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m block_ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/blockgen/utils/graph_data.py:207\u001b[39m, in \u001b[36mSchematicGraphDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> Data:\n\u001b[32m    206\u001b[39m     path = \u001b[38;5;28mself\u001b[39m.schematic_paths[index]\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     schematic = \u001b[43mload_schematic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     structure = Structure.from_schematic(schematic, source_path=path)\n\u001b[32m    209\u001b[39m     structure = _preprocess_structure(structure, \u001b[38;5;28mself\u001b[39m.config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/blockgen/utils/data_loader.py:6\u001b[39m, in \u001b[36mload_schematic\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_schematic\u001b[39m(path):\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# nbt_data = nbtlib.load(path)\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# print(nbt_data) # Prints a human-readable representation of the NBT data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     schematic_file = \u001b[43mSchematicFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m schematic_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/.venv/lib/python3.13/site-packages/nbtschematic/schematic.py:159\u001b[39m, in \u001b[36mSchematicFile.load\u001b[39m\u001b[34m(cls, filename, gzipped, byteorder)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, gzipped=\u001b[38;5;28;01mTrue\u001b[39;00m, byteorder=\u001b[33m'\u001b[39m\u001b[33mbig\u001b[39m\u001b[33m'\u001b[39m) -> \u001b[33m'\u001b[39m\u001b[33mSchematicFile\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    146\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[33;03m    Load a schematic file from disk\u001b[39;00m\n\u001b[32m    148\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m \u001b[33;03m    :return: Loaded schematic\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgzipped\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgzipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyteorder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbyteorder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/.venv/lib/python3.13/site-packages/nbtlib/nbt.py:316\u001b[39m, in \u001b[36mFile.load\u001b[39m\u001b[34m(cls, filename, gzipped, byteorder)\u001b[39m\n\u001b[32m    314\u001b[39m open_file = gzip.open \u001b[38;5;28;01mif\u001b[39;00m gzipped \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m open_file(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyteorder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/.venv/lib/python3.13/site-packages/nbtlib/nbt.py:294\u001b[39m, in \u001b[36mFile.from_fileobj\u001b[39m\u001b[34m(cls, fileobj, byteorder)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_fileobj\u001b[39m(\u001b[38;5;28mcls\u001b[39m, fileobj, byteorder=\u001b[33m\"\u001b[39m\u001b[33mbig\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load an nbt file from a proper file object.\u001b[39;00m\n\u001b[32m    277\u001b[39m \n\u001b[32m    278\u001b[39m \u001b[33;03m    The method is used by the :func:`load` helper function when the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    292\u001b[39m \u001b[33;03m            forwarded to :meth:`nbtlib.tag.Compound.parse`.\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyteorder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m.filename = \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.filename)\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.gzipped = \u001b[38;5;28misinstance\u001b[39m(fileobj, gzip.GzipFile)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/.venv/lib/python3.13/site-packages/nbtlib/tag.py:1082\u001b[39m, in \u001b[36mCompound.parse\u001b[39m\u001b[34m(cls, fileobj, byteorder)\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Override :meth:`Base.parse` for compound tags.\"\"\"\u001b[39;00m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m tag_id = \u001b[43mread_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBYTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyteorder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m tag_id != \u001b[32m0\u001b[39m:\n\u001b[32m   1084\u001b[39m     name = read_string(fileobj, byteorder)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/programming/gen/BlockGen/.venv/lib/python3.13/site-packages/nbtlib/tag.py:202\u001b[39m, in \u001b[36mread_numeric\u001b[39m\u001b[34m(fmt, fileobj, byteorder)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    201\u001b[39m     fmt = fmt[byteorder]\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fmt.unpack(\u001b[43mfileobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m)[\u001b[32m0\u001b[39m]\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m StructError:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/gzip.py:340\u001b[39m, in \u001b[36mGzipFile.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merrno\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno.EBADF, \u001b[33m\"\u001b[39m\u001b[33mread() on write-only GzipFile object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/_compression.py:68\u001b[39m, in \u001b[36mDecompressReader.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view.cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/gzip.py:546\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_member:\n\u001b[32m    543\u001b[39m     \u001b[38;5;66;03m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[32m    544\u001b[39m     \u001b[38;5;66;03m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[32m    545\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_read()\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m._size = \u001b[38;5;28mself\u001b[39m._pos\n\u001b[32m    548\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/gzip.py:515\u001b[39m, in \u001b[36m_GzipReader._read_gzip_header\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_gzip_header\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     last_mtime = \u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m last_mtime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/gzip.py:475\u001b[39m, in \u001b[36m_read_gzip_header\u001b[39m\u001b[34m(fp)\u001b[39m\n\u001b[32m    472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic != \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\037\u001b[39;00m\u001b[38;5;130;01m\\213\u001b[39;00m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadGzipFile(\u001b[33m'\u001b[39m\u001b[33mNot a gzipped file (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m % magic)\n\u001b[32m    477\u001b[39m (method, flag, last_mtime) = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m<BBIxx\u001b[39m\u001b[33m\"\u001b[39m, _read_exact(fp, \u001b[32m8\u001b[39m))\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method != \u001b[32m8\u001b[39m:\n",
      "\u001b[31mBadGzipFile\u001b[39m: Not a gzipped file (b'Ra')"
     ]
    }
   ],
   "source": [
    "special_tokens = ['<PAD>', '<BOS>', '<UNK>']\n",
    "block_counter = Counter()\n",
    "num_blocks_list = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    g = dataset[i]\n",
    "    block_ids = g.block_id[g.node_type == NODE_BLOCK].tolist()\n",
    "    block_counter.update(int(x) for x in block_ids)\n",
    "    num_blocks_list.append(len(block_ids))\n",
    "\n",
    "observed_block_ids = sorted(block_counter.keys())\n",
    "\n",
    "stoi = {tok: i for i, tok in enumerate(special_tokens)}\n",
    "for bid in observed_block_ids:\n",
    "    stoi[f'BID_{int(bid)}'] = len(stoi)\n",
    "\n",
    "itos = {v: k for k, v in stoi.items()}\n",
    "\n",
    "pad_idx = stoi['<PAD>']\n",
    "bos_idx = stoi['<BOS>']\n",
    "unk_idx = stoi['<UNK>']\n",
    "\n",
    "def block_id_to_token(block_id):\n",
    "    return stoi.get(f'BID_{int(block_id)}', unk_idx)\n",
    "\n",
    "def token_to_block_id(token):\n",
    "    label = itos.get(int(token), '<UNK>')\n",
    "    if isinstance(label, str) and label.startswith('BID_'):\n",
    "        return int(label.split('_', 1)[1])\n",
    "    return 0\n",
    "\n",
    "num_block_tokens = len(stoi)\n",
    "max_block_nodes = int(np.percentile(num_blocks_list, 95))\n",
    "max_block_nodes = max(max_block_nodes, 32)\n",
    "\n",
    "print('num block tokens:', num_block_tokens)\n",
    "print('max_block_nodes (95th pct):', max_block_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd3c22",
   "metadata": {},
   "source": [
    "## 3) Prepare train/test splits and batch collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.9 * len(indices))\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "print('train graphs:', len(train_indices), 'test graphs:', len(test_indices))\n",
    "\n",
    "\n",
    "def graph_to_block_token_sequence(g):\n",
    "    block_mask = g.node_type == NODE_BLOCK\n",
    "    block_ids = g.block_id[block_mask].tolist()\n",
    "    return [block_id_to_token(bid) for bid in block_ids]\n",
    "\n",
    "\n",
    "class GraphTokenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, id_list):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.id_list = [int(i) for i in id_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        g = self.base_dataset[self.id_list[idx]]\n",
    "        tokens = graph_to_block_token_sequence(g)\n",
    "        n_blocks = len(tokens)\n",
    "\n",
    "        if n_blocks > max_block_nodes:\n",
    "            tokens = tokens[:max_block_nodes]\n",
    "            n_blocks = max_block_nodes\n",
    "\n",
    "        decoder_input = [bos_idx] + tokens[:-1] if n_blocks > 0 else [bos_idx]\n",
    "        decoder_target = tokens if n_blocks > 0 else [unk_idx]\n",
    "\n",
    "        return {\n",
    "            'graph': g,\n",
    "            'decoder_input': torch.tensor(decoder_input, dtype=torch.long),\n",
    "            'decoder_target': torch.tensor(decoder_target, dtype=torch.long),\n",
    "            'num_blocks': torch.tensor(n_blocks, dtype=torch.long),\n",
    "            'sequence_tuple': tuple(tokens),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_graph_tokens(batch):\n",
    "    graphs = [item['graph'] for item in batch]\n",
    "    batched_graph = Batch.from_data_list(graphs)\n",
    "\n",
    "    decoder_inputs = [item['decoder_input'] for item in batch]\n",
    "    decoder_targets = [item['decoder_target'] for item in batch]\n",
    "    num_blocks = torch.stack([item['num_blocks'] for item in batch], dim=0)\n",
    "    seq_tuples = [item['sequence_tuple'] for item in batch]\n",
    "\n",
    "    max_len = max(x.shape[0] for x in decoder_inputs)\n",
    "    x_pad = torch.full((len(batch), max_len), pad_idx, dtype=torch.long)\n",
    "    y_pad = torch.full((len(batch), max_len), pad_idx, dtype=torch.long)\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(decoder_inputs, decoder_targets)):\n",
    "        n = x.shape[0]\n",
    "        x_pad[i, :n] = x\n",
    "        y_pad[i, :n] = y\n",
    "\n",
    "    return batched_graph, x_pad, y_pad, num_blocks, seq_tuples\n",
    "\n",
    "\n",
    "train_ds = GraphTokenDataset(dataset, train_indices)\n",
    "test_ds = GraphTokenDataset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_graph_tokens)\n",
    "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False, collate_fn=collate_graph_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b546aab",
   "metadata": {},
   "source": [
    "## 4) Train large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de682123",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LargePyGGraphGeneratorConfig(\n",
    "    num_block_tokens=num_block_tokens,\n",
    "    max_block_nodes=max_block_nodes,\n",
    "    hidden_dim=384,\n",
    "    latent_dim=192,\n",
    "    encoder_layers=6,\n",
    "    decoder_layers=3,\n",
    "    num_heads=6,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = LargePyGGraphGenerator(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * torch.mean(1.0 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "\n",
    "def compute_losses(output, token_targets, size_targets, pad_idx, beta=0.02):\n",
    "    logits = output['token_logits']\n",
    "    b, t, v = logits.shape\n",
    "\n",
    "    token_loss = F.cross_entropy(\n",
    "        logits.reshape(b * t, v),\n",
    "        token_targets.reshape(b * t),\n",
    "        ignore_index=pad_idx,\n",
    "    )\n",
    "\n",
    "    size_targets = torch.clamp(size_targets, min=0, max=config.max_block_nodes)\n",
    "    size_loss = F.cross_entropy(output['size_logits'], size_targets)\n",
    "\n",
    "    kl = kl_divergence(output['mu'], output['logvar'])\n",
    "    total = token_loss + 0.5 * size_loss + beta * kl\n",
    "\n",
    "    return total, token_loss, size_loss, kl\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for graph_batch, dec_inp, dec_tgt, num_blocks, _ in loader:\n",
    "        graph_batch = graph_batch.to(device)\n",
    "        dec_inp = dec_inp.to(device)\n",
    "        dec_tgt = dec_tgt.to(device)\n",
    "        num_blocks = num_blocks.to(device)\n",
    "\n",
    "        out = model(graph_batch, dec_inp)\n",
    "        total, _, _, _ = compute_losses(out, dec_tgt, num_blocks, pad_idx)\n",
    "        losses.append(float(total.item()))\n",
    "\n",
    "    return float(np.mean(losses)) if losses else 0.0\n",
    "\n",
    "\n",
    "epochs = 6\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_losses = []\n",
    "\n",
    "    for graph_batch, dec_inp, dec_tgt, num_blocks, _ in train_loader:\n",
    "        graph_batch = graph_batch.to(device)\n",
    "        dec_inp = dec_inp.to(device)\n",
    "        dec_tgt = dec_tgt.to(device)\n",
    "        num_blocks = num_blocks.to(device)\n",
    "\n",
    "        out = model(graph_batch, dec_inp)\n",
    "        total, token_loss, size_loss, kl = compute_losses(out, dec_tgt, num_blocks, pad_idx)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_losses.append(float(total.item()))\n",
    "\n",
    "    train_loss = float(np.mean(total_losses))\n",
    "    val_loss = evaluate(test_loader)\n",
    "    print(f'epoch {epoch:02d} | train={train_loss:.4f} | val={val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fa531",
   "metadata": {},
   "source": [
    "## 5) Generate novel graphs from prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_sequences = set()\n",
    "for _, _, _, _, seq_tuples in train_loader:\n",
    "    for seq in seq_tuples:\n",
    "        train_token_sequences.add(tuple(seq))\n",
    "\n",
    "\n",
    "def sequence_to_structure(token_sequence):\n",
    "    block_ids = np.array([token_to_block_id(tok) for tok in token_sequence], dtype=np.int32)\n",
    "    n = max(1, len(block_ids))\n",
    "\n",
    "    side = int(np.ceil(n ** (1 / 3)))\n",
    "    grid = np.zeros((side, side, side), dtype=np.int32)\n",
    "    grid.reshape(-1)[:n] = block_ids\n",
    "    block_data = np.zeros_like(grid, dtype=np.int32)\n",
    "    return Structure(block_ids=grid, block_data=block_data, source_path='generated_prior')\n",
    "\n",
    "\n",
    "valid_block_tokens = [stoi[f'BID_{int(bid)}'] for bid in observed_block_ids if f'BID_{int(bid)}' in stoi]\n",
    "\n",
    "novel_structures = []\n",
    "novel_sequences = []\n",
    "attempts = 0\n",
    "max_attempts = 80\n",
    "target_novel = 4\n",
    "\n",
    "while len(novel_structures) < target_novel and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "\n",
    "    z = torch.randn(1, config.latent_dim, device=device)\n",
    "    sampled_n = int(model.sample_num_blocks(z, temperature=1.0, min_blocks=8).item())\n",
    "    sampled_n = max(8, min(sampled_n, config.max_block_nodes))\n",
    "\n",
    "    token_seq = model.sample_block_tokens(\n",
    "        z,\n",
    "        bos_token_id=bos_idx,\n",
    "        num_tokens=sampled_n,\n",
    "        valid_token_ids=valid_block_tokens,\n",
    "        temperature=1.0,\n",
    "        top_k=32,\n",
    "    )\n",
    "\n",
    "    if tuple(token_seq) in train_token_sequences:\n",
    "        continue\n",
    "\n",
    "    novel_sequences.append(tuple(token_seq))\n",
    "    novel_structures.append(sequence_to_structure(token_seq))\n",
    "\n",
    "print('novel graphs generated:', len(novel_structures), 'after attempts:', attempts)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "for i, st in enumerate(novel_structures, start=1):\n",
    "    ax = fig.add_subplot(2, 2, i, projection='3d')\n",
    "    render_schem(st, ax=ax, show=False, crop_non_air=True, max_dim=20)\n",
    "    ax.set_title(f'Novel Sample {i} | shape={st.shape}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73999902",
   "metadata": {},
   "source": [
    "## 6) Compare generated vs training token distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_counter = Counter()\n",
    "for seq in train_token_sequences:\n",
    "    real_counter.update(seq)\n",
    "\n",
    "gen_counter = Counter()\n",
    "for seq in novel_sequences:\n",
    "    gen_counter.update(seq)\n",
    "\n",
    "keys = sorted(set(real_counter.keys()) | set(gen_counter.keys()))\n",
    "eps = 1e-8\n",
    "\n",
    "real = np.array([real_counter.get(k, 0) for k in keys], dtype=np.float64)\n",
    "gen = np.array([gen_counter.get(k, 0) for k in keys], dtype=np.float64)\n",
    "\n",
    "real = (real + eps) / (real.sum() + eps * len(real))\n",
    "gen = (gen + eps) / (gen.sum() + eps * len(gen))\n",
    "\n",
    "kl = float(np.sum(real * np.log(real / gen)))\n",
    "print('KL(train || generated):', round(kl, 4))\n",
    "\n",
    "top = [k for k, _ in real_counter.most_common(12)]\n",
    "labels = [itos[k] for k in top]\n",
    "rv = [real_counter[k] for k in top]\n",
    "gv = [gen_counter.get(k, 0) for k in top]\n",
    "\n",
    "x = np.arange(len(top))\n",
    "w = 0.42\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(x - w/2, rv, width=w, label='train')\n",
    "plt.bar(x + w/2, gv, width=w, label='generated')\n",
    "plt.xticks(x, labels, rotation=45)\n",
    "plt.title('Top block-token counts: train vs generated')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991555e",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Scaling knobs:\n",
    "- Increase `max_dim` in dataset\n",
    "- Increase `hidden_dim`, `encoder_layers`, `num_heads`\n",
    "- Train for more epochs\n",
    "- Increase batch size if GPU memory allows\n",
    "\n",
    "This model is graph-based, attention-based, and generates novel samples by sampling from the latent prior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
